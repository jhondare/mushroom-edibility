{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix\nimport itertools\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nimport torchvision\nfrom torchvision import transforms\nimport random\nimport shutil\n\nrandom.seed(0)\nnp.random.seed(0)\ntorch.manual_seed(0)\ntorch.cuda.manual_seed(0)\ntorch.backends.cudnn.deterministic = True","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_path = '/kaggle/input/mushrooms-classification-common-genuss-images/Mushrooms'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Temporary folders for training and test images:\nos.mkdir('/kaggle/temp')\nos.chdir('/kaggle/temp')\nos.mkdir('train')\nos.mkdir('test')\nos.chdir('/kaggle/working')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split images (75%/25%) and save to temporary folders:\nfor subfolder in os.listdir(data_path):\n\n    # Making a list of all files in current subfolder:\n    original_path = f'{data_path}/{subfolder}'\n    original_data = os.listdir(original_path)\n\n    # Number of samples in each group:\n    n_samples = len(original_data)\n    train_samples = int(n_samples * 0.75)\n        \n    train_path = f'/kaggle/temp/train/{subfolder}'\n    test_path = f'/kaggle/temp/test/{subfolder}'\n    \n    # New class subfolder for training:\n    os.chdir('/kaggle/temp/train')\n    os.mkdir(subfolder)\n    \n    # Training images:\n    for image in range(train_samples):\n        original_file = f'{original_path}/{original_data[image]}'\n        new_file = f'{train_path}/{original_data[image]}'\n        shutil.copyfile(original_file, new_file)\n    \n    # New class subfolder for testing:\n    os.chdir('/kaggle/temp/test')\n    os.mkdir(subfolder)\n    \n    # Test images:\n    for image in range(train_samples, n_samples):\n        original_file = f'{original_path}/{original_data[image]}'\n        new_file = f'{test_path}/{original_data[image]}'\n        shutil.copyfile(original_file, new_file)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.chdir('/kaggle/temp/train/Lactarius')\nprint(len([name for name in os.listdir('/kaggle/temp/train/Lactarius') if os.path.isfile(name)]))\n\nos.chdir('/kaggle/temp/test/Lactarius')\nprint(len([name for name in os.listdir('/kaggle/temp/test/Lactarius') if os.path.isfile(name)]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_root = '/kaggle/temp/train'\ntest_root = '/kaggle/temp/test'\n\nmean = np.array([0.485, 0.456, 0.406])\nstd = np.array([0.229, 0.224, 0.225])\n\ndata_transforms = transforms.Compose([\n    transforms.CenterCrop(550),\n    transforms.Resize((224,224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean, std)\n])\n\ntrain_dataset = torchvision.datasets.ImageFolder(train_root, data_transforms)\ntest_dataset = torchvision.datasets.ImageFolder(test_root, data_transforms)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 40\n\ntrain_dataloader = torch.utils.data.DataLoader(\n    train_dataset, batch_size=batch_size, shuffle=True, num_workers=batch_size)\n\ntest_dataloader = torch.utils.data.DataLoader(\n    test_dataset, batch_size=batch_size, shuffle=True, num_workers=batch_size)\n\nprint('number of batches for training:', len(train_dataloader),\n      '\\nnumber of batches for testing:', len(test_dataloader),\n      '\\nnumber of images:', len(train_dataset))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_names = [\"Agaricus\", \"Amanita\", \"Boletus\", \"Cortinarius\", \n                \"Entoloma\", \"Hygrocybe\", \"Lactarius\", \"Russula\", \"Suillus\"]\nids = [\"tensor(0)\", \"tensor(1)\", \"tensor(2)\", \"tensor(3)\", \n       \"tensor(4)\", \"tensor(5)\", \"tensor(6)\", \"tensor(7)\", \"tensor(8)\"]\n\ndict_class_names = dict(zip(ids, class_names))\nprint(dict_class_names)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show_input(input_tensor, title):\n    image = input_tensor.permute(1, 2, 0).numpy()\n    image = std * image + mean\n    plt.imshow(image.clip(0, 1))\n    plt.title(title)\n    plt.show()\n    plt.pause(0.001)\n\nX_batch, y_batch = next(iter(train_dataloader))\n\nfor x_item, y_item in zip(X_batch, y_batch):\n    show_input(x_item, dict_class_names[str(y_item)])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class LeNet(torch.nn.Module):\n    def __init__(self):\n        super(LeNet, self).__init__()\n\n        self.conv1 = torch.nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5, padding=2)\n        self.act1 = torch.nn.ReLU()\n        self.maxpool1 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n\n        self.conv2 = torch.nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, padding=0)\n        self.act2 = torch.nn.ReLU()\n        self.maxpool2 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n        \n        self.flatten = torch.nn.Flatten()\n        self.dropout = torch.nn.Dropout(p=0.25, inplace=False)\n        \n        self.fc1 = torch.nn.Linear(in_features=46656, out_features=120)\n        self.act3 = torch.nn.ReLU()\n        self.fc2 = torch.nn.Linear(in_features=120, out_features=84)\n        self.act4 = torch.nn.ReLU()\n        self.fc3 = torch.nn.Linear(in_features=84, out_features=9)\n    \n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.act1(x)\n        x = self.maxpool1(x)\n\n        x = self.conv2(x)\n        x = self.act2(x)\n        x = self.maxpool2(x)\n\n        #x = x.view(x.size(0), x.size(1) * x.size(2) * x.size(3))\n        #x = x.view(-1, self.num_flat_features(x))\n        x = self.flatten(x)\n        x = self.dropout(x)\n        \n        x = self.fc1(x)\n        x = self.act3(x)\n        x = self.fc2(x)\n        x = self.act4(x)\n        x = self.fc3(x)\n        return x\n\nlenet = LeNet().to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\n\ndef train_model(model, num_epochs):\n    \n    loss = torch.nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=1.0e-3)\n\n    # Decay LR by a factor of 0.1 every 7 epochs\n    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.3)\n    \n    for epoch in range(num_epochs):\n        print('Epoch {}/{}:'.format(epoch, num_epochs - 1), flush=True)\n        \n        start_time = time.time()\n\n        model.train()  # Set model to training mode\n\n        running_loss = 0.\n        running_acc = 0.\n\n        # Iterate over data.\n        for inputs, labels in train_dataloader:\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            \n            optimizer.zero_grad()\n\n            # forward and backward\n            with torch.set_grad_enabled(True):\n                preds = model(inputs)\n                loss_value = loss(preds, labels)\n                preds_class = preds.argmax(dim=1)\n\n                # backward + optimize only if in training phase\n                loss_value.backward()\n                optimizer.step()\n\n            # statistics\n            running_loss += loss_value.item()\n            running_acc += (preds_class == labels.data).float().mean()\n\n        epoch_loss = running_loss / len(train_dataloader)\n        epoch_acc = running_acc / len(train_dataloader)\n\n        scheduler.step()\n        \n        print('Loss: {:.4f} Acc: {:.4f} Time: {:.4f}'.format(epoch_loss, epoch_acc, time.time() - start_time), flush=True)\n\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchvision import models\n\n\n#------------------------------------------\n# ResNet152\n#------------------------------------------\nresnet152 = models.resnet152(pretrained=True)\n\nfor param in resnet152.parameters():\n    param.requires_grad = False\n    \nresnet152.fc = torch.nn.Sequential(\n    torch.nn.Linear(resnet152.fc.in_features, 256),\n    torch.nn.ReLU(),\n    torch.nn.Linear(256, 9)\n)\n\nresnet152 = resnet152.to(device)\n#------------------------------------------\n\n\n#------------------------------------------\n# MobileNet\n#------------------------------------------\nmobilenet = models.mobilenet_v2(pretrained=True)\n\nfor param in mobilenet.parameters():\n    param.requires_grad = False\n\nmobilenet.classifier[1] = torch.nn.Sequential(\n    torch.nn.Linear(mobilenet.classifier[1].in_features, 256),\n    torch.nn.ReLU(),\n    torch.nn.Linear(256, 9)\n)\n\nmobilenet = mobilenet.to(device)\n#------------------------------------------","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from PIL import ImageFile\nImageFile.LOAD_TRUNCATED_IMAGES = True","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Training ResNet152\nresnet152 = train_model(resnet152, num_epochs=50)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Training Mobilenet\nmobilenet = train_model(mobilenet, num_epochs=50)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Training lenet\nlenet = train_model(lenet, num_epochs=10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def accuracy(model):\n    model.eval()\n    correct = 0\n    \n    for inputs, labels in test_dataloader:\n        \n        inputs = inputs.to(device)\n        labels = labels.to(device)\n        \n        with torch.set_grad_enabled(False):\n            preds = model(inputs)\n            \n            for label, pred in zip(labels, preds):\n                label = int(label.data.cpu().numpy())\n                pred = int(torch.argmax(pred).data.cpu().numpy())\n                if label == pred:\n                    correct += 1\n\n    return correct / len(test_dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Accuraces\nprint(\"ResNet's accuracy: \", accuracy(resnet152))\nprint(\"MobileNet's accuracy: \", accuracy(mobilenet))\nprint(\"LeNet's accuracy: \", accuracy(lenet))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_target_and_prediction(model):\n    model.eval()\n\n    targets = []\n    predictions = []\n\n    for inputs, labels in test_dataloader:\n        \n        inputs = inputs.to(device)\n        labels = labels.to(device)\n        \n        with torch.set_grad_enabled(False):\n            preds = model(inputs)\n            \n            for label, pred in zip(labels, preds):\n                label = int(label.data.cpu().numpy())\n                pred = int(torch.argmax(pred).data.cpu().numpy())\n            \n                targets.append(label)\n                predictions.append(pred)\n    \n    return targets, predictions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_confusion_matrix(model):\n    targets, preditions = get_target_and_prediction(model)\n    cm = confusion_matrix(targets, preditions)\n    return cm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_confusion_matrix(model, normalize=False):\n    cm = create_confusion_matrix(model)\n    cmap = plt.cm.Blues\n    \n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n    \n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.colorbar()\n    tick_marks = np.arange(len(class_names))\n    plt.xticks(tick_marks, class_names, rotation=45)\n    plt.yticks(tick_marks, class_names)\n    \n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#ResNet's confusion matrix\nplt.figure(figsize=(10,10))\nplot_confusion_matrix(resnet152)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#MobileNet's confusion matrix\nplt.figure(figsize=(10,10))\nplot_confusion_matrix(mobilenet)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#LeNet's confusion matrix\nplt.figure(figsize=(10,10))\nplot_confusion_matrix(lenet)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}